<! Made with jot using doc2html -style main.css>
<link rel="stylesheet" href="main.css" type="text/css">


</UL><H2><A NAME="JOT TESTS">JOT Tests</H2>
<P>This document describes the pre-release tests used to verify a new version of jot. These are regression tests - they cannot prove that a version of jot is correct, they just test for regressions in specific areas.</P>

<P>Typically, a new version of jot will be assigned the name jot_dev (or jot_dev.exe for a windows version).</P>


</UL><H3><A NAME="TEST.JOT">test.jot</H3>
<PRE>
$ ./jot_dev $&#123;JOT_RESOURCES&#125;/t.t -in=%r=test  
</PRE>
<P>This runs basic regression tests on all jot commands and command variants. It is totally autonomous and, on completion, should display the message</P>

<UL><P>"Successfully completed all tests"</P>


</UL><H3><A NAME="TEST_VISUAL.JOT">test_visual.jot</H3>
<PRE>
$ ./jot_dev t.t -in=%r=test_visual
</PRE>
<P>This runs a series of tests designed to detect any regression in the way it drives the screen. In olden times this was literally a visual test - the intrepid tester had to sit and check each several dozen screenshots to make sure nothing was going wrong. These days, though, it's all done automatically using the <A HREF="#QUERY WINDOW">query window</A> command.</P>


</UL><H3><A NAME="MONKEY_TEST.JOT">monkey_test.jot</H3>
<PRE>
$ ./jot_dev t.t -in='n.a$z$i"%% -tests=100000 -exit" %r=monkey_test'
</PRE>
<P>This constructs test scripts containing thousands of randomly-generated valid but meaningless commands. These are designed to detect any crashyness. As one script completes successfully it constructs a new one using a different random sequence of commands. </P>

<P>The monkey_test.jot script can also be set up to generate smaller scripts of, maybe only a few hundred commands, for checking with valgrind - see http:\\valgrind.org for details.</P>

<P>At the start of day, it resets the pseudorandom generator using the current date-time stamp, the value of this seed is reflected in the name of the generated script. To regenerate a script it is possible to force it to take a predefined seed with the -seed=N modifier.</P>

<P>The monkey_test-generated scripts can be very large - shorter scripts are less effective since they are less likely to contain combinations of commands that might trigger some hidden data sensitivity. To identify the commands actively provoking the crash there is a script <A HREF="#ERROR_SEARCH.JOT">error_search.jot</A> that does a binary search of the script, typically boiling it down to 10 lines or less. However searching very large scripts is impracticable due to the enormous search times - about 10000 tests per script is a pretty good compromise.</P>

<P>Valid monkey_test.jot qualifiers.</P>

<UL><P>-seed=&#060;value&#062; </P>

<P>uses the predefined seed (by default, it constructs one from current time) exits after completing the new script.</P>

<P>-script=&#060;pathName&#062; </P>

<P>runs the preexisting command script and exits on completion.</P>

<P>-tests=&#060;n&#062; </P>

<P>specifies number of tests to be generated in each test script - defaults to 10000.</P>

<P>-trace=&#060;x&#062; </P>

<P>specifies trace mode (in hex) at start of test run, defaults to 6002.</P>

<P>-head="&#060;commands&#062;" </P>

<P>specifies a command sequence to be applied after normal initialization.</P>

<P>-tail="&#060;commands&#062;" </P>

<P>specifies a command sequence to be applied before normal test-script exit.</P>

<P>-subprocess </P>

<P>Launches test in a subprocess</P>

<P>-commandcounter=n </P>

<P>Inserts a command to set the command counter before launching script in the subprocess.</P>

<P>-noloop </P>

<P>exits after first test script (-noloop is implicit with -script=... or -seed=... options.</P>

<P>-label </P>

<P>Labels each command line with %%Line &#060;lineNo&#062;</P>

<P>-pause </P>

<P>enters debugger on completion of each pass.</P>

<P>-nowin </P>

<P>Suppresses window view.</P>

<P>-subprocess</P>

<P>Launches test in a subprocess</P>

<P>-xterm</P>

<P>Launches test in an xterm</P>

<P>-subprocess</P>

<P>Launches test in a subprocess</P>

<P>-gdb</P>

<P>Launches test in a subprocess supervised by gdb</P>

<P>-valgrind</P>

<P>Launches test in a subprocess supervised by valgrind</P>


</UL><H3><A NAME="ERROR_SEARCH.JOT">error_search.jot</H3>
<PRE>
$ jot $&#123;JOT_RESOURCES&#125;/l99.t -in="%r=error_search &#091;-gdb|-valgrind|-xterm&#093;
</PRE>
<UL><P>&#091; -failif=&#060;jotCommands&#062;&#093;</P>

<P>-script=&#060;scriptPath1&#062;&#091; +&#060;scriptPath2&#062;&#091; +...&#093;&#093;"</P>

</UL><P>When a script generated by monkey_test.jot fails - either it's crashed or, when used with valgrind, it's done something naughty but not immediately fatal, revealing a data sensitive bug. By analysing the generated script we might be able to spot what's going on, maybe with the aid of gdb or a similar debugger.</P>

<P>By default, the test sessions are launched as a simple subprocess, the -xterm qualifier launches it in an xterm so you can keep an eye on what's happening, the -gdb qualifier launches the test monitored by gdb session and -valgrind monitors with valgrind.</P>

<P>The error_search script detects that it misbehaved by searching the reply from the child process. You can specify a different failure criterion by specifing your own comparator commands in the -failif modifier.</P>

<P>Often it's not that obvious what's going on and it would be useful to know exactly which commands are trigger the error. Now, it's trivial to find which was the last command before it crashed but, several earlier commands were involved in creating the conditions for the crash. Normally, it's less than a dozen or so commands out of the many thousands of randomly-chosen commands. The error_search.jot script performs a binary search, slicing down the generated script and repeating with progressively smaller slices, until it finds the minimum required to provoke the error.</P>

<P>Demonstration of error_search</P>

<UL><P><LI>First generate a script with a randomly-placed error, the -crash option is only used to generate a crashing script for this purpose:</P></LI>

<PRE>
$ jot t.t -st -ini="%r=monkey_test -crash -tests=1000 -seed=1234567890"
</PRE>
<P><LI>Now search the script for line(s) causing the crash:</P></LI>

<PRE>
$ jot t.t -in="%r=error_search -gdb -script=test1234567890.jot"
</PRE>
<P><LI>This should generate the script test1234567890.jot_pruned - a copy of your original script containing only the lines contribution to the error condition - in this case just the line inserted by the -crash option to monkey_test:</P></LI>

<PRE>
o@ol123 oo/%n/   %%Crash now.
</PRE>
</UL><P>Typical usage:</P>

<PRE>
$ ./jot t.t -in="%r=error_search -valgrind -script=./test108110917.jot"
</PRE>
<P>The error_search.jot script runs your test script (in this case test108110917.jot) and, if the full script results in valgrind error reports, the script is whittled down to the minimum set of commands that still results in valgrind errors.</P>


</UL><H3><A NAME="TEST.SH">test.sh</H3>
<PRE>
$ ./test.sh
</PRE>
<P>Some simple tests for the linux version in simulated real-life situations.</P>

<UL><P><LI>Test 1 - <A HREF="#TEST.JOT">test.jot</A> </P></LI>

<P><LI>Test 2 - Streaming out to stdout</P></LI>

<P><LI>Test 3 - Accepting input from stdin stream - in -tty mode.</P></LI>

<P><LI>Test 3a - Accepting input from stdin stream - in non-tty mode.</P></LI>

<P><LI>Test 4 - Accepting input from stream and piping to stdout - in -tty mode.</P></LI>

<P><LI>Test 5 - Read from command, -tty mode</P></LI>

<P><LI>Test 5a - Read from command - in non-tty mode.</P></LI>

<P><LI>Test 6 - Filter mode -tty mode</P></LI>

<P><LI>Test 6a - Filter mode - in non-tty mode</P></LI>

<P><LI>Test 7 - streamed-in commands -tty mode</P></LI>

<P><LI>Test 7a - streamed-in commands - in non-tty mode</P></LI>

<P><LI>Test 8 - journal-keeping and recovery</P></LI>

<P><LI>Test 9 - -asConsole</P></LI>

<P><LI>Test 10 - get.jot - tests the get.jot script in the most important configurations.</P></LI>


</UL><H3><A NAME="TEST.BAT">test.bat</H3>
<P>This runs tests similar to <A HREF="#TEST.SH">test.sh</A> but in order to sidestep some problems with wine, the tests are set up and results compared by the calling script (either <A HREF="#TEST_WIN.SH">test_win.sh</A> or <A HREF="#TEST_WIN.BAT">test_win.bat</A>).</P>

<P>Note that this script does not run directly from a wine or windows console - it is called by either <A HREF="#TEST_WIN.BAT">test_win.bat</A> (wine/windows) or <A HREF="#TEST_WIN.SH">test_win.sh</A> (from linux, using a wine console).</P>


</UL><H3><A NAME="TEST_WIN.SH">test_win.sh</H3>
<PRE>
$ test_win.sh
</PRE>
<P>This runs the windows version of the command-line test script <A HREF="#TEST.BAT">test.bat</A> but from a linux command line, using a wine console.</P>


</UL><H3><A NAME="TEST_WIN.BAT">test_win.bat</H3>
<PRE>
$ test_win
</PRE>
<P>This runs the windows version of the command-line test script <A HREF="#TEST.BAT">test.bat</A> from a windows (or wine) console.</P>

